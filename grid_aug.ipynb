{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf2b2a0-daa8-459d-8973-15ed881c8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 01:31:40.970106: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-24 01:31:40.970155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-24 01:31:40.971731: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-24 01:31:40.980135: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from keras import Sequential, Model, layers, callbacks, optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
    "# import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58225141-f6b1-4311-ae83-d807cf20f204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       AnEar  lactation  dim  milk_production  MilkShif    Dur\n",
      "0        123          4    1             11.5     11.50  276.0\n",
      "1        123          4    2             48.4     24.20  384.0\n",
      "2        123          4    3             66.4     33.20  672.0\n",
      "3        123          4    4             57.1     28.55  354.0\n",
      "4        123          4    5             78.5     39.25  396.0\n",
      "...      ...        ...  ...              ...       ...    ...\n",
      "25841   9977          2  301             72.8     36.40  540.0\n",
      "25842   9977          2  302             73.0     36.50  600.0\n",
      "25843   9977          2  303             70.9     35.45  540.0\n",
      "25844   9977          2  304             70.6     35.30  516.0\n",
      "25845   9977          2  305             67.7     33.85  522.0\n",
      "\n",
      "[25846 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "milk_df = pd.read_csv(\"milk.csv\", parse_dates=[\"Date\"])\n",
    "milk_df.dropna(how='any', axis=0, inplace=True, ignore_index=True)\n",
    "df = milk_df.drop('Date', axis=1)\n",
    "df = df.drop('dim2', axis=1)\n",
    "df = df.drop('Cond', axis=1)\n",
    "df = df.drop('Peak', axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d931835b-a12b-4200-9ebe-7e685218444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 123 1009 1017 1209 1218 1242 1243 1275 1280 1305 1327 1329 1341 1362\n",
      " 1368 1387 1393 7761 8064 8334 8385 8393 8434 8435 8438 8509 8581 8667\n",
      " 8723 8732 8741 8932 8947 8996 9005 9066 9069 9078 9081 9090 9101 9149\n",
      " 9188 9207 9284 9346 9363 9388 9389 9431 9463 9464 9478 9487 9488 9525\n",
      " 9529 9549 9558 9562 9583 9885 9891 9892 9894 9908 9909 9910 9917 9918\n",
      " 9920 9922 9926 9928 9929 9933 9939 9943 9944 9946 9951 9961 9967 9968\n",
      " 9977]\n"
     ]
    }
   ],
   "source": [
    "dv_treino = {}\n",
    "dv_teste = {}\n",
    "nomes_vacas = df['AnEar'].unique()\n",
    "print(nomes_vacas)\n",
    "for nome in nomes_vacas:\n",
    "    vals = df[df['AnEar'] == nome].values\n",
    "    vals = np.delete(vals, 0, axis=1)\n",
    "    dv_treino[nome] = vals[:50]\n",
    "    dv_teste[nome] = vals[50:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e0a7b76-8b3e-4b76-b26d-f88b6576da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_treino_aug = {}\n",
    "for nome in nomes_vacas:\n",
    "    new_vals = []\n",
    "    vals = dv_treino[nome]\n",
    "    for i in range(0, len(vals)-1):\n",
    "        new_vals.append(vals[i])\n",
    "        new_line = [(vi+vf)/2 for vi, vf in zip(vals[i], vals[i+1])]\n",
    "        new_vals.append(new_line)\n",
    "    new_vals.append(vals[len(vals)-1])\n",
    "\n",
    "    dv_treino_aug[nome] = np.array(new_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f152f2f0-cc98-4a1b-9855-e88afe48a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, hidden_layer_sizes, dropout, learning_rate, loss):\n",
    "  model=Sequential()\n",
    "  model.add(layers.InputLayer((input_size,)))\n",
    "  model.add(layers.Flatten())\n",
    "  model.add(layers.Dense(hidden_layer_sizes, activation=\"relu\", kernel_initializer=\"he_uniform\"))\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Dense(hidden_layer_sizes, activation=\"relu\", kernel_initializer=\"he_uniform\"))\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Dense(hidden_layer_sizes, activation=\"relu\", kernel_initializer=\"he_uniform\"))\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.BatchNormalization())\n",
    "  model.add(layers.Dense(hidden_layer_sizes, activation=\"relu\", kernel_initializer=\"he_uniform\"))\n",
    "  model.add(layers.Dropout(dropout))\n",
    "  model.add(layers.Dense(1,activation=\"linear\"))\n",
    "\n",
    "  #final_model = Model([model.input], out)\n",
    "  model.compile(loss=loss, optimizer=optimizers.Adam(learning_rate=learning_rate), metrics=['mae'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31c5245-63b6-42f1-9ddf-2cd290239e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 01:31:43.102542: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.114137: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.117574: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fe70b51-50e4-477f-a247-610d16291c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_aug_values = np.concatenate(list(dv_treino_aug.values()), axis=0)\n",
    "x_train_aug = np.delete(all_aug_values, 2, axis=1)\n",
    "y_train_aug = all_aug_values[:, 2].reshape(-1, 1)\n",
    "\n",
    "x_aug_mmc = MinMaxScaler().fit(x_train_aug)\n",
    "x_train_aug = x_aug_mmc.transform(x_train_aug)\n",
    "\n",
    "y_aug_mmc = MinMaxScaler().fit(y_train_aug)\n",
    "y_train_aug = y_aug_mmc.transform(y_train_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633378e1-6866-426e-aa35-217f0cc76d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_values = np.concatenate(list(dv_teste.values()), axis=0)\n",
    "x_test = np.delete(all_test_values, 2, axis=1)\n",
    "y_test = all_test_values[:, 2].reshape(-1, 1)\n",
    "\n",
    "x_aug_teste = x_aug_mmc.transform(x_test)\n",
    "y_aug_teste = y_aug_mmc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9544b41-d8d2-428b-ab7a-04d4ae4039e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 01:31:43.162482: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.166265: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.169863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.490242: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.492448: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.494450: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-05-24 01:31:43.496339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13764 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2024-05-24 01:31:46.928506: I external/local_xla/xla/service/service.cc:168] XLA service 0x7faf4105f990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-24 01:31:46.928541: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-05-24 01:31:46.936019: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-24 01:31:46.960906: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1716514307.065714  209372 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIM - 1/54  -  256  -  0.01  -  0.3  -  16\n",
      "FIM - 2/54  -  256  -  0.01  -  0.3  -  32\n",
      "FIM - 3/54  -  256  -  0.01  -  0.4  -  16\n",
      "FIM - 4/54  -  256  -  0.01  -  0.4  -  32\n",
      "FIM - 5/54  -  256  -  0.01  -  0.5  -  16\n",
      "FIM - 6/54  -  256  -  0.01  -  0.5  -  32\n",
      "FIM - 7/54  -  256  -  0.001  -  0.3  -  16\n",
      "FIM - 8/54  -  256  -  0.001  -  0.3  -  32\n",
      "FIM - 9/54  -  256  -  0.001  -  0.4  -  16\n",
      "FIM - 10/54  -  256  -  0.001  -  0.4  -  32\n",
      "FIM - 11/54  -  256  -  0.001  -  0.5  -  16\n",
      "FIM - 12/54  -  256  -  0.001  -  0.5  -  32\n",
      "FIM - 13/54  -  256  -  0.0001  -  0.3  -  16\n",
      "FIM - 14/54  -  256  -  0.0001  -  0.3  -  32\n",
      "FIM - 15/54  -  256  -  0.0001  -  0.4  -  16\n",
      "FIM - 16/54  -  256  -  0.0001  -  0.4  -  32\n",
      "FIM - 17/54  -  256  -  0.0001  -  0.5  -  16\n",
      "FIM - 18/54  -  256  -  0.0001  -  0.5  -  32\n",
      "FIM - 19/54  -  512  -  0.01  -  0.3  -  16\n",
      "FIM - 20/54  -  512  -  0.01  -  0.3  -  32\n",
      "FIM - 21/54  -  512  -  0.01  -  0.4  -  16\n",
      "FIM - 22/54  -  512  -  0.01  -  0.4  -  32\n",
      "FIM - 23/54  -  512  -  0.01  -  0.5  -  16\n",
      "FIM - 24/54  -  512  -  0.01  -  0.5  -  32\n",
      "FIM - 25/54  -  512  -  0.001  -  0.3  -  16\n",
      "FIM - 26/54  -  512  -  0.001  -  0.3  -  32\n",
      "FIM - 27/54  -  512  -  0.001  -  0.4  -  16\n",
      "FIM - 28/54  -  512  -  0.001  -  0.4  -  32\n",
      "FIM - 29/54  -  512  -  0.001  -  0.5  -  16\n",
      "FIM - 30/54  -  512  -  0.001  -  0.5  -  32\n",
      "FIM - 31/54  -  512  -  0.0001  -  0.3  -  16\n",
      "FIM - 32/54  -  512  -  0.0001  -  0.3  -  32\n",
      "FIM - 33/54  -  512  -  0.0001  -  0.4  -  16\n",
      "FIM - 34/54  -  512  -  0.0001  -  0.4  -  32\n",
      "FIM - 35/54  -  512  -  0.0001  -  0.5  -  16\n",
      "FIM - 36/54  -  512  -  0.0001  -  0.5  -  32\n",
      "FIM - 37/54  -  784  -  0.01  -  0.3  -  16\n"
     ]
    }
   ],
   "source": [
    "resultados = {\n",
    "    \"hidden_layer_sizes\": [], \n",
    "    \"learnRate\": [],\n",
    "    \"dropout\": [],\n",
    "    \"batchSize\": [],\n",
    "    \"MAE\": [], \n",
    "    \"MSE\": [],\n",
    "}\n",
    "\n",
    "seed = 101\n",
    "x_treino, x_valid, y_treino, y_valid = train_test_split(x_train_aug, y_train_aug, test_size=0.3, random_state=seed)\n",
    "i = 1\n",
    "for hls in [256, 512, 784]:\n",
    "    for lr in [0.01, 0.001, 0.0001]:\n",
    "        for do in [0.3, 0.4, 0.5]:\n",
    "            for bs in [16, 32]:\n",
    "                modelo_aug = get_model(input_size=4, hidden_layer_sizes=hls, dropout=do, learning_rate=lr, loss='mean_absolute_error')\n",
    "                name = \"aug\"\n",
    "                hist_aug = modelo_aug.fit(\n",
    "                    x_treino,\n",
    "                    y_treino,\n",
    "                    batch_size=bs,\n",
    "                    epochs=50,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[\n",
    "                        ModelCheckpoint(filepath=f\"pesos/{name}_{hls}_{lr}_{do}_bs.keras\",save_best_only=True,verbose=0)\n",
    "                    ],\n",
    "                    verbose=0\n",
    "                )\n",
    "                pred_aug = modelo_aug.predict(x_aug_teste, verbose=0)\n",
    "                resultados[\"MAE\"].append(mean_absolute_error(y_aug_teste, pred_aug))\n",
    "                resultados[\"MSE\"].append(mean_squared_error(y_aug_teste, pred_aug))\n",
    "                resultados[\"hidden_layer_sizes\"].append(hls)\n",
    "                resultados[\"learnRate\"].append(lr)\n",
    "                resultados[\"dropout\"].append(do)\n",
    "                resultados[\"batchSize\"].append(bs)\n",
    "                print(f\"FIM - {i}/54  -  {hls}  -  {lr}  -  {do}  -  {bs}\")\n",
    "                i+=1"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
